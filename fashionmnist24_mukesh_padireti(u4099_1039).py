# -*- coding: utf-8 -*-
"""fashionmnist24-Mukesh_Padireti(U4099-1039).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1__DwZFYuQYYhTDn6jpo3vEZSSaZRyPJM

Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.

Each training and test example is assigned to one of the following labels:

Label	Description
0	T-shirt/top
1	Trouser
2	Pullover
3	Dress
4	Coat
5	Sandal
6	Shirt
7	Sneaker
8	Bag
9	Ankle boot

Class counts are all 1,000:
{0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}
"""

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

# You cannot modify from here until it is indicated by a comment
(test_data),test_data_info=tfds.load('fashion_mnist',split='test',with_info=True,as_supervised=True)

(train_data),ds_info=tfds.load('fashion_mnist',split=['train[3000:57000]'],with_info=True,as_supervised=True)



def getnewtst():
  (new_test),new_test_info=tfds.load('fashion_mnist',split=['train[0:2999]'],with_info=True,as_supervised=True)
  new_test = new_test[0].map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
  new_test = new_test.batch(64)
  new_test = new_test.cache()
  new_test = new_test.prefetch(tf.data.AUTOTUNE)
  return new_test


# Can modify code now below this comment Though be careful if you change the
# normalization

def normalize_img(image, label):
  """Normalizes images: `uint8` -> `float32`.
  The model wants the float and tfds gives you 0-255."""
  return tf.cast(image, tf.float32) / 255., label

#------------Analysis------------#
"""
Mukesh Padireti.

At First i have started looking for the drawbacks in this model, the drawbacks are:

The optimzer:
   * The SGD with momemtum is not dynamic with learning rate it is static,
     So if the learning rate is not dynamic the weights will remains same for the entire training process.
   * Althought the momemtum can help in accelerating convergence, it is not more usefull unless the learning rate is dynamic.

 Not using Regularization and Early Stopping:
     * Both the Regularization and early stopping helps in preventing the Overfitting, the way they work is different.
     * The Regularization works by adding penalty to the loss loss function, Which can stop model dependency on a specific weights.
     * The Early Stopping works by stopping the training process if the model does not increase its accuracy after specific number epoch(Patience level).
     * Using the both Regularization and Early Stopping is helpfull, while using the Regularization keeps the model Robust on the unseen data
       and the Early Stopping prevents more iteration(epoch).

Network Architecture:
     * The model uses only one convolutional layer by which the model cannot capture complex pattern.
     * I tried to Agument the data by adding more images by flipping the image, rotating the image, and zooming the image, and it has added unnecessary noise.
       At first i have increased the number of convolutional layer and changed aptimzer and tried to agumenting the data it failed to improve the accuracy
       and also it reduced the accuracy to a lot(34percent). For the second attempt i have added the learning schedule increased the
       number of epoch and added the early stopping and the Regularization and changed the optimizer..etc, after by doing all these the
       accuracy went from 34 to 66 percent which is still low.

       Reason:
         * I think it happend because fashionmnist already have over 60,000 examples, By agumenting it is not creating new patterns rather confusion the model,
           also the Architecture is simple it failed to take advantage of the Agumentation.

"""
#-------------------------------------#

train_data = train_data[0].map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
train_data = train_data.cache()
train_data = train_data.shuffle(ds_info.splits['train'].num_examples)
train_data = train_data.batch(64)
train_data = train_data.prefetch(tf.data.AUTOTUNE)


test_data = test_data.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
test_data = test_data.batch(128)
test_data = test_data.cache()
test_data = test_data.prefetch(tf.data.AUTOTUNE)

# Learning rate scheduler, It will adjust the learning rate after every epoch, and make sure the model does not stuck at local minimum.
learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 0.001 * 0.7 ** (epoch) #Training with decay rate of 0.7
)

# Early stopping is added to avoid overfitting
early_stopping_using_callBack = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',  # moniters the validation loss.
    patience=5,  # it stops if there is no improvement after 5 epochs.
    restore_best_weights=True  # capture the best weights.
)

#------------Analysis-----------#
# here i have tried using different functions like sigmiod and leaky_relu to check how they work for this model.
# The relu had the better convergence then these two(sigmiod and leaky_relu) functions.
# I am using dropout for the both Convolutional layer and Dense layer, adding Dropout to Convolutional layer prevents model dependency on specify features.
# Adding Dropout to Dense layer makes the network to use different layer combinations for training.
#-------------------------------#

model = tf.keras.models.Sequential([
    # First Conv2D layer
     tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),
     tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    # Second Conv2D layer added to increase the number of layers.
    tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
    tf.keras.layers.Dropout(0.3), # Dropout with 30 percent
    tf.keras.layers.Flatten(),

    # Dense layer with regularization
    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    tf.keras.layers.Dropout(0.2),  # Dropout with 20 percent
    tf.keras.layers.Dense(10, activation='softmax')
])

#-----------Analysis------------#
# Here i am using Adam optimzer instead of the Stochastic gradient descent.
# The Adam aptimzer can dynamically change the learning rates and can speeding up convergence.
# For learning_rate i am trying to fine tune it by using different values like(0.001,0.003,0.005. here i am using 5 epochs to test the accuracy for different rates.
#for 0.001-The accuracy is 88.55
#for 0.003-The accuracy is 88.42
#for 0.005-The accurcy is 87.44
#As the learning rate increases the model converge less effectively, this shows by increasing the learning rates also updates the large weights which lead to lower results.
#--------------------------------#
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Using Adam optimzer
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

#----------Analysis---------#
# Here i am Training the model with 10 epochs instead of 3, traing with more epochs will give model more time to learn from the data.
# By using early stopping if there is no improvement after certains epochs the training will stops which can helps to avoid overfitting.
history = model.fit(
    train_data,
    epochs=10, #Training with 10 epochs
    validation_data=test_data,
    callbacks=[learning_rate_scheduler, early_stopping_using_callBack]
)

#--------------Analysis----------#
# The accuracy before changes is 84.73 and after the changes the accuracy increased to 90.62 percent
#--------------------------------#

#----------outputs------------------:
"""
Epoch 1/10
/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:635: UserWarning: "`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?
  output, from_logits = _get_logits(
844/844 ━━━━━━━━━━━━━━━━━━━━ 99s 106ms/step - loss: 0.8856 - sparse_categorical_accuracy: 0.7392 - val_loss: 0.4755 - val_sparse_categorical_accuracy: 0.8670 - learning_rate: 0.0010
Epoch 2/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 127s 96ms/step - loss: 0.4731 - sparse_categorical_accuracy: 0.8608 - val_loss: 0.4319 - val_sparse_categorical_accuracy: 0.8719 - learning_rate: 7.0000e-04
Epoch 3/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 86s 101ms/step - loss: 0.4164 - sparse_categorical_accuracy: 0.8765 - val_loss: 0.3862 - val_sparse_categorical_accuracy: 0.8878 - learning_rate: 4.9000e-04
Epoch 4/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 84s 100ms/step - loss: 0.3729 - sparse_categorical_accuracy: 0.8879 - val_loss: 0.3687 - val_sparse_categorical_accuracy: 0.8892 - learning_rate: 3.4300e-04
Epoch 5/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 135s 92ms/step - loss: 0.3468 - sparse_categorical_accuracy: 0.8945 - val_loss: 0.3419 - val_sparse_categorical_accuracy: 0.8980 - learning_rate: 2.4010e-04
Epoch 6/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 84s 94ms/step - loss: 0.3354 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.3274 - val_sparse_categorical_accuracy: 0.9041 - learning_rate: 1.6807e-04
Epoch 7/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 79s 90ms/step - loss: 0.3149 - sparse_categorical_accuracy: 0.9057 - val_loss: 0.3198 - val_sparse_categorical_accuracy: 0.9021 - learning_rate: 1.1765e-04
Epoch 8/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 82s 90ms/step - loss: 0.3040 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3129 - val_sparse_categorical_accuracy: 0.9036 - learning_rate: 8.2354e-05
Epoch 9/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 78s 93ms/step - loss: 0.2970 - sparse_categorical_accuracy: 0.9105 - val_loss: 0.3075 - val_sparse_categorical_accuracy: 0.9062 - learning_rate: 5.7648e-05
Epoch 10/10
844/844 ━━━━━━━━━━━━━━━━━━━━ 77s 91ms/step - loss: 0.2940 - sparse_categorical_accuracy: 0.9109 - val_loss: 0.3043 - val_sparse_categorical_accuracy: 0.9062 - learning_rate: 4.0354e-05
"""

'''
# This code checks the count per class and each training has 60K
# Each class in test also has 1K for a total of 10K
def checkclasscnt(name,mysplit):
  ds,ds_info = tfds.load(name,split=mysplit,with_info=True)
  print(ds_info.features["label"].names)
# Get the number of classes
  num_classes = ds_info.features['label'].num_classes

# Create a dictionary to store class counts
  class_counts = {i: 0 for i in range(num_classes)}


  for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`

   label = example["label"]
   class_counts[label.numpy()] += 1
  return(class_counts)


ccount=checkclasscnt('fashion_mnist','train')
print(ccount)
'''